{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from algorithms import get_features, plot_durations, get_audio, get_trim, pad_audio, remove_noise\n",
    "import librosa\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate audios path and read CSVs to get features for both development and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the addresses in corresponding variables\n",
    "data_path = \"dsl_data/development.csv\"\n",
    "evaluation_path = \"dsl_data/evaluation.csv\"\n",
    "\n",
    "# Importing development and evaluation data from csv files\n",
    "df = pd.read_csv(data_path)\n",
    "evaluation_df = pd.read_csv(evaluation_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of current language: ['English (United States)']\n",
      "Unique values of first language: ['English (United States)']\n",
      "Unique values of fluency level: ['native']\n"
     ]
    }
   ],
   "source": [
    "# Discover that evaluation data only contains English (United States) and Native speakers\n",
    "print(\"Unique values of current language:\",evaluation_df['Current language used for work/school'].unique())\n",
    "print(\"Unique values of first language:\",evaluation_df['First Language spoken'].unique())\n",
    "print(\"Unique values of fluency level:\",evaluation_df['Self-reported fluency level '].unique())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data not found in EVALUATION set, rows where first & current language is not English (United States) and fluency is not native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_df = df[df['First Language spoken'] == 'English (United States)']\n",
    "modified_df = modified_df[modified_df['Current language used for work/school'] == 'English (United States)']\n",
    "modified_df = modified_df[modified_df['Self-reported fluency level '] == 'native']\n",
    "\n",
    "df = modified_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding (Development & Evaluation)\n",
    " \n",
    "transform categorical data into numerical values since sklearn doesn't accept strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "\n",
    "le_mapping = {}\n",
    "encoding_columns = ['Self-reported fluency level ','First Language spoken','Current language used for work/school','ageRange','gender']\n",
    "\n",
    "# Encode DEVELOPMENT DATA \n",
    "for col in encoding_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_mapping[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Encode EVALUATION DATA TOO\n",
    "for col, mapping in le_mapping.items():\n",
    "    evaluation_df[col] = evaluation_df[col].map(mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Decoding (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding \n",
    "\n",
    "# for col, mapping in le_mapping.items():\n",
    "#     df[col] = df[col].map(mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine \"action\" & \"object\" in one dataframe column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action-object\"] = df['action'].astype(str) +\"-\"+ df[\"object\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load audio files (.wav)\n",
    "\n",
    "used to load audio files at specific path and return the waveform as a numpy array data, along with the sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df['path'].apply(get_audio))\n",
    "evaluation_df = evaluation_df.join(evaluation_df['path'].apply(get_audio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim audio files\n",
    "\n",
    "reduce the duration of audio files by removing part of each audio signal that contains silence or \"noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum duration in both sets: 10.32126984126984s\n",
      "Ceil maximum duration in both sets: 11s\n"
     ]
    }
   ],
   "source": [
    "df = df.join(df[['data', 'sample_rate']].apply(get_trim, axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df[['data', 'sample_rate']].apply(get_trim, axis=1))\n",
    "\n",
    "# Exract maximum length to use in padding later\n",
    "max_development = df['duration_trim'].max()\n",
    "max_evaluation = evaluation_df['duration_trim'].max()\n",
    "maximum_duration = np.maximum(max_development, max_evaluation)\n",
    "print(f\"Maximum duration in both sets: {maximum_duration}s\")\n",
    "maximum_duration = math.ceil(maximum_duration)\n",
    "print(f\"Ceil maximum duration in both sets: {maximum_duration}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise reduction in python using spectral gating (noisereduce 2.0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.apply(lambda x: remove_noise(x['data_trim'], x['sample_rate']), axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df.apply(lambda x: remove_noise(x['data_trim'], x['sample_rate']), axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad audio files to max audio length in both sets\n",
    "\n",
    "adds silence (zero values) to the audio signals to make all audio signals same fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.apply(lambda x: pad_audio(x['data_clean'], x['sample_rate'],maximum_duration), axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df.apply(lambda x: pad_audio(x['data_clean'], x['sample_rate'],maximum_duration), axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply get_features function to extract features from audio files and trim silence from beggining and end of each audio file\n",
    "(Done together to reduce complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.apply(lambda x: get_features(x['data_clean'], x['sample_rate']), axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df.apply(lambda x: get_features(x['data_clean'], x['sample_rate']), axis=1))\n",
    "\n",
    "# # Save extracted features in csv files to avoid repeating steps \n",
    "df.to_csv('save_csv/training.csv')\n",
    "evaluation_df.to_csv('save_csv/evaluation.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: Get features from previously saved csv files to prevent loading and extracting features again (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'save_csv/training.csv').iloc[:,1:]\n",
    "# evaluation_df = pd.read_csv(r'save_csv/evaluation.csv').iloc[:,1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Id','path','speakerId','action','object','action-object','Self-reported fluency level ','First Language spoken','Current language used for work/school','data', 'sample_rate', 'duration', 'data_trim', 'duration_trim', 'data_pad', 'duration_pad','data_clean'],axis=1)\n",
    "x_evaluation = evaluation_df.drop(['Id','path','speakerId','Self-reported fluency level ','First Language spoken','Current language used for work/school','data', 'sample_rate', 'duration', 'data_trim', 'duration_trim', 'data_pad', 'duration_pad','data_clean'],axis=1)\n",
    "\n",
    "y = df[['action-object']].copy()\n",
    "\n",
    "# Change column names from Int to Str to avoid error by SKLEARN\n",
    "x.columns = x.columns.astype(str)\n",
    "x_evaluation.columns = x_evaluation.columns.astype(str)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns/features (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = [str(i) for i in range(210, 978)]\n",
    "# x = x.drop(columns=column_names)\n",
    "# x_evaluation= x_evaluation.drop(columns=column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data: MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply min-max scaler to make all features between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x) \n",
    "X_scaled = scaler.transform(x)\n",
    "X_evaluation_scaled = scaler.transform(x_evaluation)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% training data and 20% test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y,test_size = .2,random_state = 42, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {'n_estimators': [10, 100, 1000],\n",
    "              'max_depth': [None, 3, 5, 7],\n",
    "              'min_samples_leaf': [1, 2, 3],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# Initialize the random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=2, scoring='accuracy')\n",
    "grid_search.fit(X_scaled, np.ravel(y))\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Random Forest Classifier after obtaining best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5143923240938166\n",
      "Precision: 0.5553643242974984\n",
      "Recall: 0.5143923240938166\n",
      "F1-Score: 0.504275886954889\n"
     ]
    }
   ],
   "source": [
    "#Create a Classifier\n",
    "rf_clf=RandomForestClassifier(criterion ='gini', max_depth= None, min_samples_leaf= 2, n_estimators= 100)\n",
    "\n",
    "#Train the model using the training sets \n",
    "rf_clf.fit(x_train,np.ravel(y_train))\n",
    "y_pred_rf=rf_clf.predict(x_test)\n",
    "\n",
    "# Model Accuracy using test data (20%)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf,average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf,average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_rf,average='weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid to search over\n",
    "\n",
    "param_grid = {'C': [0.1,1,4,8,10,50], 'gamma': [0.01, 0.1, 1]}\n",
    "\n",
    "# Create a SVM with an RBF kernel\n",
    "svm = SVC(kernel='rbf')\n",
    "\n",
    "# Perform the grid search using 10-fold cross-validation\n",
    "grid_search = GridSearchCV(svm, param_grid)\n",
    "grid_search.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# Print the best parameters and the corresponding mean test score\n",
    "print(\"Best parameters: \",grid_search.best_params_)\n",
    "print(\"Best score: \",grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SVM Classifier after obtaining best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6497867803837953\n",
      "Precision: 0.6532943846487141\n",
      "Recall: 0.6497867803837953\n",
      "F1-Score: 0.6504952476272321\n"
     ]
    }
   ],
   "source": [
    "svm_clf = svm.SVC(kernel='rbf', C=4,gamma=0.1)\n",
    "\n",
    "svm_clf.fit(x_train,np.ravel(y_train))\n",
    "y_pred_svm=svm_clf.predict(x_test)\n",
    "\n",
    "# Model Accuracy using test data (20%)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_svm,average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_svm,average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_svm,average='weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels of evaluation data using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new SVM model to fit all data\n",
    "svm_clf = svm.SVC(kernel='rbf', C=4,gamma=0.1)\n",
    "svm_clf.fit(X_scaled,np.ravel(y))\n",
    "\n",
    "evaluation_svm=svm_clf.predict(X_evaluation_scaled)\n",
    "evaluation_svm = list(map(lambda s: s.replace(\"-\", \"\"), evaluation_svm))\n",
    "\n",
    "svm_df = pd.DataFrame(evaluation_svm, columns = ['Predicted'])\n",
    "svm_df.index.name = 'Id'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels of evaluation data using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new random forest model to fit all data\n",
    "rf_clf=RandomForestClassifier(criterion ='gini', max_depth= None, min_samples_leaf= 2, n_estimators= 100)\n",
    "rf_clf.fit(X_scaled,np.ravel(y))\n",
    "\n",
    "evaluation_rf=rf_clf.predict(X_evaluation_scaled)\n",
    "\n",
    "evaluation_rf = list(map(lambda s: s.replace(\"-\", \"\"), evaluation_rf))\n",
    "\n",
    "rf_df = pd.DataFrame(evaluation_rf, columns = ['Predicted'])\n",
    "rf_df.index.name = 'Id'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both predictions in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/svm_predictions1675290887.csv\n",
      "evaluation/rf_predictions1675290887.csv\n"
     ]
    }
   ],
   "source": [
    "# Get current timestamp\n",
    "now = int(time.time())\n",
    "\n",
    "# Save evaluation predictions of both models in csv files\n",
    "svm_df.to_csv(f'evaluation/svm_predictions{now}.csv',index=True,header=True)\n",
    "rf_df.to_csv(f'evaluation/rf_predictions{now}.csv',index=True,header=True)\n",
    "\n",
    "# Print paths of saved csv\n",
    "print(f'evaluation/svm_predictions{now}.csv')\n",
    "print(f'evaluation/rf_predictions{now}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a6207722f3be5c8aeec7a028becc5489fbd28a1705747404dde5b96b7146bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
