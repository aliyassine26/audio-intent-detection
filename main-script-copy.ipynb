{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from algorithms import get_features, plot_durations, get_audio, get_trim, pad_audio\n",
    "import encode_dict_data \n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the addresses in corresponding variables\n",
    "data_path = \"dsl_data/development.csv\"\n",
    "evaluation_path = \"dsl_data/evaluation.csv\"\n",
    "\n",
    "# Importing development and evaluation data from csv files\n",
    "df = pd.read_csv(data_path)\n",
    "evaluation_df = pd.read_csv(evaluation_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover that evaluation data only contains English (United States) and Native speakers\n",
    "print(\"Unique values of current language:\",evaluation_df['Current language used for work/school'].unique())\n",
    "print(\"Unique values of first language:\",evaluation_df['First Language spoken'].unique())\n",
    "print(\"Unique values of fluency level:\",evaluation_df['Self-reported fluency level '].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE all rows where first & current language is not English (United States) and fluency is not native\n",
    "modified_df = df[df['First Language spoken'] == 'English (United States)']\n",
    "modified_df = modified_df[modified_df['Current language used for work/school'] == 'English (United States)']\n",
    "modified_df = modified_df[modified_df['Self-reported fluency level '] == 'native']\n",
    "\n",
    "df = modified_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Label Encoding (Development & Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "\n",
    "le_mapping = {}\n",
    "encoding_columns = ['Self-reported fluency level ','First Language spoken','Current language used for work/school','ageRange','gender']\n",
    "\n",
    "# Encode DEVELOPMENT DATA \n",
    "for col in encoding_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_mapping[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Encode EVALUATION DATA TOO\n",
    "for col, mapping in le_mapping.items():\n",
    "    evaluation_df[col] = evaluation_df[col].map(mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Decoding (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding \n",
    "\n",
    "# for col, mapping in le_mapping.items():\n",
    "#     df[col] = df[col].map(mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine \"action\" & \"object\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:10]\n",
    "evaluation_df = evaluation_df.iloc[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action-object\"] = df['action'].astype(str) +\"-\"+ df[\"object\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load audio files (.wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df['path'].apply(get_audio))\n",
    "evaluation_df = evaluation_df.join(evaluation_df['path'].apply(get_audio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df[['data', 'sample_rate']].apply(get_trim, axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df[['data', 'sample_rate']].apply(get_trim, axis=1))\n",
    "\n",
    "# Exract maximum length to use in padding later\n",
    "max_development = df['duration_trim'].max()\n",
    "max_evaluation = evaluation_df['duration_trim'].max()\n",
    "maximum_duration = np.maximum(max_development, max_evaluation)\n",
    "print(f\"Maximum duration in both sets: {maximum_duration}s\")\n",
    "maximum_duration = math.ceil(maximum_duration)\n",
    "print(f\"Ceil maximum duration in both sets: {maximum_duration}s\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.apply(lambda x: pad_audio(x['data_trim'], x['sample_rate'],maximum_duration), axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df.apply(lambda x: pad_audio(x['data_trim'], x['sample_rate'],maximum_duration), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply get_features function to extract features from audio files and trim silence from beggining and end of each audio file\n",
    "(Done together to reduce complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.apply(lambda x: get_features(x['data_pad'], x['sample_rate']), axis=1))\n",
    "evaluation_df = evaluation_df.join(evaluation_df.apply(lambda x: get_features(x['data_pad'], x['sample_rate']), axis=1))\n",
    "\n",
    "# # Save extracted features in csv files to avoid repeating steps \n",
    "# df.to_csv('save_csv/training.csv')\n",
    "# evaluation_df.to_csv('save_csv/evaluation.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR get features from previously saved csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import dataframes from previously saved csv files so we don't need to get features again\n",
    "\n",
    "# df = pd.read_csv(r'save_csv/training.csv').iloc[:,1:]\n",
    "# evaluation_df = pd.read_csv(r'save_csv/evaluation.csv').iloc[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['action-object'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Id','path','speakerId','action','object','action-object','Self-reported fluency level ','First Language spoken','Current language used for work/school','data', 'sample_rate', 'duration', 'data_trim', 'duration_trim', 'data_pad', 'duration_data'],axis=1)\n",
    "x_evaluation = evaluation_df.drop(['Id','path','speakerId','Self-reported fluency level ','First Language spoken','Current language used for work/school','data', 'sample_rate', 'duration', 'data_trim', 'duration_trim', 'data_pad', 'duration_data'],axis=1)\n",
    "\n",
    "y = df[['action-object']].copy()\n",
    "\n",
    "# Change column names from Int to Str to avoid error by SKLEARN\n",
    "x.columns = x.columns.astype(str)\n",
    "x_evaluation.columns = x_evaluation.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA().fit(data_rescaled)\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# xi = np.arange(1, 213, step=1)\n",
    "# y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.ylim(0.0,1.1)\n",
    "# plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.xticks(np.arange(0, 212, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "# plt.ylabel('Cumulative variance (%)')\n",
    "# plt.title('The number of components needed to explain variance')\n",
    "\n",
    "# plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "# plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "# ax.grid(axis='x')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling (Z-score) for PCA since pca is sensitive to the scale of features.\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# scaler.fit(x)\n",
    "# X_scaled = scaler.transform(x)\n",
    "# X_evaluation_scaled = scaler.transform(x_evaluation)  # apply same transformation to test data\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=.95).fit(X_scaled)\n",
    "# X_pca = pca.transform(X_scaled)\n",
    "# X_evaluation_pca = pca.transform(X_evaluation_scaled)\n",
    "# print(sum(pca.explained_variance_ratio_)) \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x) \n",
    "X_scaled = scaler.transform(x)\n",
    "X_evaluation_scaled = scaler.transform(x_evaluation)  # apply same transformation to test data\n",
    "\n",
    "# def standardize(x_train, x_test):\n",
    "#     mean = np.mean(x_train, axis=0)\n",
    "#     std = np.std(x_train, axis=0)\n",
    "#     x_train_standardized = (x_train - mean) / std\n",
    "#     x_test_standardized = (x_test - mean) / std\n",
    "#     return x_train_standardized, x_test_standardized\n",
    "# X_scaled, X_evaluation_scaled = standardize(x, x_evaluation)\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=.95).fit(X_scaled)\n",
    "# X_pca = pca.transform(X_scaled)\n",
    "# X_evaluation_pca = pca.transform(X_evaluation_scaled)\n",
    "# print(sum(pca.explained_variance_ratio_)) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% training data and 20% test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y,test_size = .2,random_state = 42, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a Classifier\n",
    "rf_clf=RandomForestClassifier(criterion ='entropy', max_depth= 7, min_samples_leaf= 2, n_estimators= 1000)\n",
    "\n",
    "\n",
    "#Train the model using the training sets \n",
    "rf_clf.fit(x_train,np.ravel(y_train))\n",
    "y_pred_rf=rf_clf.predict(x_test)\n",
    "\n",
    "# Model Accuracy using test data (20%)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf,average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf,average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_rf,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_clf = svm.SVC(kernel = 'rbf', C=8)\n",
    "# svm_clf = svm.SVC()\n",
    "\n",
    "\n",
    "\n",
    "svm_clf.fit(x_train,np.ravel(y_train))\n",
    "y_pred_svm=svm_clf.predict(x_test)\n",
    "\n",
    "\n",
    "# Model Accuracy using test data (20%)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_svm,average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_svm,average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred_svm,average='weighted'))\n",
    "\n",
    "# 0.646\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid to search over\n",
    "# param_grid = {'n_estimators': [10, 100, 1000],\n",
    "#               'max_depth': [3, 5, 7],\n",
    "#               'min_samples_leaf': [1, 2, 3],\n",
    "#               'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# # Initialize the random forest classifier\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Use GridSearchCV to find the best parameters\n",
    "# grid_search = GridSearchCV(rf, param_grid, cv=2, scoring='accuracy')\n",
    "# grid_search.fit(X_scaled, y)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best parameters: \", grid_search.best_params_)\n",
    "# print(\"Best score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid to search over\n",
    "\n",
    "# param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "\n",
    "# # Create a SVM with an RBF kernel\n",
    "# svm = SVC(kernel='rbf')\n",
    "\n",
    "# # Perform the grid search using 10-fold cross-validation\n",
    "# grid_search = GridSearchCV(svm, param_grid)\n",
    "# grid_search.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# # Print the best parameters and the corresponding mean test score\n",
    "# print(\"Best parameters: \",grid_search.best_params_)\n",
    "# print(\"Best score: \",grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels of evaluation data using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_svm =svm_clf.predict(X_evaluation_scaled)\n",
    "\n",
    "evaluation_svm = list(map(lambda s: s.replace(\"-\", \"\"), evaluation_svm))\n",
    "\n",
    "svm_df = pd.DataFrame(evaluation_svm, columns = ['Predicted'])\n",
    "svm_df.index.name = 'Id'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels of evaluation data using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_rf=rf_clf.predict(X_evaluation_scaled)\n",
    "\n",
    "evaluation_rf = list(map(lambda s: s.replace(\"-\", \"\"), evaluation_rf))\n",
    "\n",
    "rf_df = pd.DataFrame(evaluation_rf, columns = ['Predicted'])\n",
    "rf_df.index.name = 'Id'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both predictions in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = int(time.time())\n",
    "\n",
    "svm_df.to_csv(f'evaluation/svm_predictions{now}.csv',index=True,header=True)\n",
    "rf_df.to_csv(f'evaluation/rf_predictions{now}.csv',index=True,header=True)\n",
    "\n",
    "# Print paths of saved csv\n",
    "print(f'evaluation/svm_predictions{now}.csv')\n",
    "print(f'evaluation/rf_predictions{now}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d632ece9ffddb0af8003b7fd18727f76c563889d1c26c69682c14ed7f3fb0566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
